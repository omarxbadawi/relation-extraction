{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wUr6CrINcUfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc564d8b-d67d-48b5-a8ba-670e39535208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train split info:\n",
            "\tNum. relations (excl. no relation): 80\n",
            "\tAvg. samples per relation (excl. no relation): 560\n",
            "\tNum. no relation samples: 3296\n",
            "\n",
            "Validation split info:\n",
            "\tNum. relations (excl. no relation): 80\n",
            "\tAvg. samples per relation (excl. no relation): 70\n",
            "\tNum. no relation samples: 412\n",
            "\n",
            "Test split info:\n",
            "\tNum. relations (excl. no relation): 80\n",
            "\tAvg. samples per relation (excl. no relation): 70\n",
            "\tNum. no relation samples: 412\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install sacremoses -q\n",
        "\n",
        "import itertools\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "from sacremoses import MosesDetokenizer\n",
        "\n",
        "# Only using the Wikipedia-based data splits\n",
        "INPUT_DATA_SPLITS = [\n",
        "    \"raw/train_wiki.json\",\n",
        "    \"raw/val_wiki.json\"\n",
        "]\n",
        "\n",
        "OUTPUT_DATA_SPLITS = {\n",
        "    \"train\": \"train.json\",\n",
        "    \"val\": \"val.json\",\n",
        "    \"test\": \"test.json\"\n",
        "}\n",
        "\n",
        "RANDOM_SEED = 41332\n",
        "VAL_TEST_SPLIT = [0.1, 0.1]\n",
        "\n",
        "NO_RELATION_ID = \"NA\"\n",
        "NO_RELATION_ORDER_DEPENDENT = True # If True, will generate both (a, b) and\n",
        "                                   # (b, a) samples. If false, will only\n",
        "                                   # generate one of the two.\n",
        "\n",
        "DETOKENIZE_SAMPLES = True\n",
        "\n",
        "ADD_ENTITY_DELIM_TOKENS = True\n",
        "ENTITY_DELIM_TOKENS = {\n",
        "    \"h\": (\"[E1S]\", \"[E1E]\"),\n",
        "    \"t\": (\"[E2S]\", \"[E2E]\")\n",
        "}\n",
        "\n",
        "# Aggregate splits into single dataset\n",
        "with open(INPUT_DATA_SPLITS[0], \"r\") as f:\n",
        "    full_dataset = json.load(f)\n",
        "for i in range(1, len(INPUT_DATA_SPLITS)):\n",
        "    with open(INPUT_DATA_SPLITS[i], \"r\") as f:\n",
        "        dataset = json.load(f)\n",
        "        for relation, samples in dataset.items():\n",
        "            full_dataset.setdefault(relation, [])\n",
        "            full_dataset[relation].extend(samples)\n",
        "\n",
        "# Remove entity type information (don't think this is useful to us)\n",
        "for samples in full_dataset.values():\n",
        "    for sample in samples:\n",
        "        del sample[\"h\"][1]\n",
        "        del sample[\"t\"][1]\n",
        "\n",
        "# Identify entities and relations per token sequence\n",
        "tokens_to_entities = {}\n",
        "tokens_to_relations = {}\n",
        "for samples in full_dataset.values():\n",
        "    for sample in samples:\n",
        "        tokens = tuple(sample[\"tokens\"])\n",
        "        tokens_to_entities.setdefault(tokens, {}).update({\n",
        "            sample[\"h\"][0]: sample[\"h\"],\n",
        "            sample[\"t\"][0]: sample[\"t\"]\n",
        "        })\n",
        "        tokens_to_relations.setdefault(tokens, set()).add(\n",
        "            (sample[\"h\"][0], sample[\"t\"][0])\n",
        "        )\n",
        "\n",
        "# Add no relation (NA) class to dataset\n",
        "if NO_RELATION_ORDER_DEPENDENT:\n",
        "    generator = itertools.permutations\n",
        "else:\n",
        "    generator = itertools.combinations\n",
        "for tokens, entities in tokens_to_entities.items():\n",
        "    for a, b in generator(entities.keys(), 2):\n",
        "        if (\n",
        "            (a, b) not in tokens_to_relations[tokens]\n",
        "            and (b, a) not in tokens_to_relations[tokens]\n",
        "        ):\n",
        "            full_dataset.setdefault(NO_RELATION_ID, []).append({\n",
        "                \"tokens\": list(tokens),\n",
        "                \"h\": entities[a],\n",
        "                \"t\": entities[b]\n",
        "            })\n",
        "\n",
        "# Detokenize samples using Moses detokenizer if required\n",
        "if DETOKENIZE_SAMPLES:\n",
        "    detokenizer = MosesDetokenizer()\n",
        "    for samples in full_dataset.values():\n",
        "        for sample in samples:\n",
        "\n",
        "            # Find all entity positions\n",
        "            entity_poses = []\n",
        "            for entity_id in [\"h\", \"t\"]:\n",
        "                entity = sample[entity_id][0]\n",
        "                for poses in sample[entity_id][1]:\n",
        "                    entity_poses.append(\n",
        "                        (entity_id, entity, poses[0], len(poses))\n",
        "                    )\n",
        "            entity_poses.sort(key=lambda x: x[2])\n",
        "\n",
        "            # Check for nested entities\n",
        "            for i, entity_pos in enumerate(entity_poses[:-1]):\n",
        "                _, _, start, length = entity_pos\n",
        "                if start + length > entity_poses[i + 1][2]:\n",
        "                    raise ValueError(\n",
        "                        \"Overlapping or nested entities are not supported for \"\n",
        "                        f\"detokenization, found here: {entity_poses}\"\n",
        "                    )\n",
        "\n",
        "            # Detokenize entities seperately\n",
        "            for entity_id, entity, start, length in reversed(entity_poses):\n",
        "                detokenized = detokenizer.detokenize(\n",
        "                    sample[\"tokens\"][start:start + length]\n",
        "                )\n",
        "\n",
        "                # Add entity delimiter tokens if needed\n",
        "                if ADD_ENTITY_DELIM_TOKENS:\n",
        "                    detokenized = (\n",
        "                        f\"{ENTITY_DELIM_TOKENS[entity_id][0]}\"\n",
        "                        f\" {detokenized} \"\n",
        "                        f\"{ENTITY_DELIM_TOKENS[entity_id][1]}\"\n",
        "                    )\n",
        "\n",
        "                # Add detokenized entity back into token list\n",
        "                sample[\"tokens\"] = (\n",
        "                    sample[\"tokens\"][:start] +\n",
        "                    [detokenized] +\n",
        "                    sample[\"tokens\"][start + length:]\n",
        "                )\n",
        "\n",
        "            # Detokenize entire sample\n",
        "            sample[\"tokens\"] = detokenizer.detokenize(sample[\"tokens\"])\n",
        "\n",
        "            # Remove irrelevant information\n",
        "            for entity_id in [\"h\", \"t\"]:\n",
        "                sample[entity_id] = sample[entity_id][0]\n",
        "\n",
        "# Set random seed\n",
        "randomiser = random.Random(RANDOM_SEED)\n",
        "\n",
        "# Split full dataset into new train/val/test splits\n",
        "train_split, val_split, test_split = {}, {}, {}\n",
        "for relation, samples in full_dataset.items():\n",
        "    randomiser.shuffle(samples)\n",
        "    val_samples, test_samples, train_samples = np.split(samples, [\n",
        "            int(VAL_TEST_SPLIT[0] * len(samples)),\n",
        "            int((VAL_TEST_SPLIT[0] + VAL_TEST_SPLIT[1]) * len(samples))\n",
        "    ])\n",
        "    train_split.setdefault(relation, []).extend(train_samples)\n",
        "    val_split.setdefault(relation, []).extend(val_samples)\n",
        "    test_split.setdefault(relation, []).extend(test_samples)\n",
        "\n",
        "# Save new data splits\n",
        "with open(OUTPUT_DATA_SPLITS[\"train\"], \"w\") as f:\n",
        "    json.dump(train_split, f)\n",
        "with open(OUTPUT_DATA_SPLITS[\"val\"], \"w\") as f:\n",
        "    json.dump(val_split, f)\n",
        "with open(OUTPUT_DATA_SPLITS[\"test\"], \"w\") as f:\n",
        "    json.dump(test_split, f)\n",
        "\n",
        "# Print debug information\n",
        "for split_name, split in ([\n",
        "    (\"Train\", train_split),\n",
        "    (\"Validation\", val_split),\n",
        "    (\"Test\", test_split)\n",
        "]):\n",
        "    print(\n",
        "        f\"{split_name} split info:\"\n",
        "        f\"\\n\\tNum. relations (excl. no relation): {len(split) - 1}\"\n",
        "        f\"\\n\\tAvg. samples per relation (excl. no relation): {int(np.mean([len(samples) for relation, samples in split.items() if relation != NO_RELATION_ID]))}\"\n",
        "        f\"\\n\\tNum. no relation samples: {len(split[NO_RELATION_ID])}\\n\"\n",
        "    )\n"
      ]
    }
  ]
}