{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing\n",
        "\n",
        "The [FewRel](https://github.com/thunlp/FewRel) dataset, which we are planning to use, was intended for a slightly different relation extraction task than ours. The intended task involved learning new relations dynamically and the dataset has been pre-split into training and validation *(and testing, although this isn't publicly available)* subsets accordingly, this means that the relations found in the validation split are not present in the training split.\n",
        "\n",
        "Our task is slightly different, and easier, as our model only needs to learn a fixed set of relations and we don't have to consider learning new relations dynamically after training. Therefore, we need to modify how the dataset has been split to ensure all relations appear in all subsets *(and we also have to create our own testing split)*.\n",
        "\n",
        "Running the cell below will re-split the dataset to more suit our needs and save the new training, validation, and testing splits to disk. There are also options available to do some further preprocessing on the data if desired, as described below.\n",
        "\n",
        "## Options\n",
        "\n",
        "*   **INPUT_DATA_SPLITS** - File paths of the original dataset splits to include in the re-split dataset.\n",
        "\n",
        "*   **OUTPUT_DATA_SPLITS** - File paths to save the new training, validation, and testing splits to.\n",
        "\n",
        "*   **RANDOM_SEED** - Random seed used when dividing the samples into splits.\n",
        "\n",
        "*   **VAL_TEST_SPLIT** - Proportion of samples to use in the new validation and testing splits.\n",
        "\n",
        "*   **ADD_NO_RELATION_SAMPLES** - Whether to add a new \"no relation\" class, and samples, to the dataset (as it is not included in the original data). Samples for this new class are found by searching the dataset for entities defined within the same token sequence but without any defined relation.\n",
        "\n",
        " *   **NO_RELATION_ID** - ID to use for the new \"no relation\" class.\n",
        "\n",
        " *   **NO_RELATION_ORDER_DEPENDENT** - Whether the new \"no relation\" class is dependent on entity order. I.e. if true, both (a, b) and (b, a) samples will be generated for the class, if false, only one of the two will be generated.\n",
        "\n",
        "*   **DETOKENIZE_SAMPLES** - Whether to detokenize the samples, as they come pre-tokenized in the dataset. I recommend keeping this set to true, since we don't know which tokenizer was originally used, thus making it hard to tokenize new inputs after training.\n",
        "\n",
        "*   **ADD_ENTITY_DELIM_TOKENS** - Whether to add delimiter tokens around entities within each sample. E.g. \"The [E1S] dog [E1E] chased the [E2S] cat [E2E]\".\n",
        "\n",
        " *   **ENTITY_DELIM_TOKENS** - Start and end delimiter tokens to use for each entity type.\n",
        "\n"
      ],
      "metadata": {
        "id": "czyImyrT25YU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wUr6CrINcUfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf44ce53-a553-4a0a-cc32-817d9275df80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train split info:\n",
            "\tNum. relations (excl. no relation): 80\n",
            "\tAvg. samples per relation (excl. no relation): 560\n",
            "\tNum. no relation samples: 3296\n",
            "\n",
            "Validation split info:\n",
            "\tNum. relations (excl. no relation): 80\n",
            "\tAvg. samples per relation (excl. no relation): 70\n",
            "\tNum. no relation samples: 412\n",
            "\n",
            "Test split info:\n",
            "\tNum. relations (excl. no relation): 80\n",
            "\tAvg. samples per relation (excl. no relation): 70\n",
            "\tNum. no relation samples: 412\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install sacremoses -q\n",
        "\n",
        "import itertools\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "from sacremoses import MosesDetokenizer\n",
        "\n",
        "\n",
        "# === OPTIONS ==================================================================\n",
        "\n",
        "INPUT_DATA_SPLITS = (\n",
        "    \"raw/train_wiki.json\",\n",
        "    \"raw/val_wiki.json\"\n",
        ")\n",
        "\n",
        "OUTPUT_DATA_SPLITS = {\n",
        "    \"train\": \"train.json\",\n",
        "    \"val\": \"val.json\",\n",
        "    \"test\": \"test.json\"\n",
        "}\n",
        "\n",
        "RANDOM_SEED = 41332\n",
        "\n",
        "VAL_TEST_SPLIT = (0.1, 0.1)\n",
        "\n",
        "ADD_NO_RELATION_SAMPLES = True\n",
        "NO_RELATION_ID = \"NA\"\n",
        "NO_RELATION_ORDER_DEPENDENT = True\n",
        "\n",
        "DETOKENIZE_SAMPLES = True\n",
        "\n",
        "ADD_ENTITY_DELIM_TOKENS = True\n",
        "ENTITY_DELIM_TOKENS = {\n",
        "    \"h\": (\"[E1S]\", \"[E1E]\"),\n",
        "    \"t\": (\"[E2S]\", \"[E2E]\")\n",
        "}\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# Aggregate splits into single dataset\n",
        "with open(INPUT_DATA_SPLITS[0], \"r\") as f:\n",
        "    full_dataset = json.load(f)\n",
        "for i in range(1, len(INPUT_DATA_SPLITS)):\n",
        "    with open(INPUT_DATA_SPLITS[i], \"r\") as f:\n",
        "        dataset = json.load(f)\n",
        "        for relation, samples in dataset.items():\n",
        "            full_dataset.setdefault(relation, [])\n",
        "            full_dataset[relation].extend(samples)\n",
        "\n",
        "# Remove entity type information (don't think this is useful to us)\n",
        "for samples in full_dataset.values():\n",
        "    for sample in samples:\n",
        "        del sample[\"h\"][1]\n",
        "        del sample[\"t\"][1]\n",
        "\n",
        "# Identify and add no relation samples to dataset\n",
        "if ADD_NO_RELATION_SAMPLES:\n",
        "\n",
        "    # Identify entities and relations per token sequence\n",
        "    tokens_to_entities = {}\n",
        "    tokens_to_relations = {}\n",
        "    for samples in full_dataset.values():\n",
        "        for sample in samples:\n",
        "            tokens = tuple(sample[\"tokens\"])\n",
        "            tokens_to_entities.setdefault(tokens, {}).update({\n",
        "                sample[\"h\"][0]: sample[\"h\"],\n",
        "                sample[\"t\"][0]: sample[\"t\"]\n",
        "            })\n",
        "            tokens_to_relations.setdefault(tokens, set()).add(\n",
        "                (sample[\"h\"][0], sample[\"t\"][0])\n",
        "            )\n",
        "\n",
        "    # Add no relation class to dataset\n",
        "    if NO_RELATION_ORDER_DEPENDENT:\n",
        "        generator = itertools.permutations\n",
        "    else:\n",
        "        generator = itertools.combinations\n",
        "    for tokens, entities in tokens_to_entities.items():\n",
        "        for a, b in generator(entities.keys(), 2):\n",
        "            if (\n",
        "                (a, b) not in tokens_to_relations[tokens]\n",
        "                and (b, a) not in tokens_to_relations[tokens]\n",
        "            ):\n",
        "                full_dataset.setdefault(NO_RELATION_ID, []).append({\n",
        "                    \"tokens\": list(tokens),\n",
        "                    \"h\": entities[a],\n",
        "                    \"t\": entities[b]\n",
        "                })\n",
        "\n",
        "# Detokenize samples using Moses detokenizer if required\n",
        "if DETOKENIZE_SAMPLES:\n",
        "    detokenizer = MosesDetokenizer()\n",
        "    for samples in full_dataset.values():\n",
        "        for sample in samples:\n",
        "\n",
        "            # Find all entity positions\n",
        "            entity_poses = []\n",
        "            for entity_id in [\"h\", \"t\"]:\n",
        "                entity = sample[entity_id][0]\n",
        "                for poses in sample[entity_id][1]:\n",
        "                    entity_poses.append(\n",
        "                        (entity_id, entity, poses[0], len(poses))\n",
        "                    )\n",
        "            entity_poses.sort(key=lambda x: x[2])\n",
        "\n",
        "            # Check for nested entities\n",
        "            for i, entity_pos in enumerate(entity_poses[:-1]):\n",
        "                _, _, start, length = entity_pos\n",
        "                if start + length > entity_poses[i + 1][2]:\n",
        "                    raise ValueError(\n",
        "                        \"Overlapping or nested entities are not supported for \"\n",
        "                        f\"detokenization, found here: {entity_poses}\"\n",
        "                    )\n",
        "\n",
        "            # Detokenize entities seperately\n",
        "            for entity_id, entity, start, length in reversed(entity_poses):\n",
        "                detokenized = detokenizer.detokenize(\n",
        "                    sample[\"tokens\"][start:start + length]\n",
        "                )\n",
        "\n",
        "                # Add entity delimiter tokens if required\n",
        "                if ADD_ENTITY_DELIM_TOKENS:\n",
        "                    detokenized = (\n",
        "                        f\"{ENTITY_DELIM_TOKENS[entity_id][0]}\"\n",
        "                        f\" {detokenized} \"\n",
        "                        f\"{ENTITY_DELIM_TOKENS[entity_id][1]}\"\n",
        "                    )\n",
        "\n",
        "                # Add detokenized entity back into token list\n",
        "                sample[\"tokens\"] = (\n",
        "                    sample[\"tokens\"][:start] +\n",
        "                    [detokenized] +\n",
        "                    sample[\"tokens\"][start + length:]\n",
        "                )\n",
        "\n",
        "            # Detokenize entire sample\n",
        "            sample[\"tokens\"] = detokenizer.detokenize(sample[\"tokens\"])\n",
        "\n",
        "            # Remove irrelevant information\n",
        "            for entity_id in [\"h\", \"t\"]:\n",
        "                sample[entity_id] = sample[entity_id][0]\n",
        "\n",
        "# Set random seed\n",
        "randomiser = random.Random(RANDOM_SEED)\n",
        "\n",
        "# Split full dataset into new train/val/test splits\n",
        "train_split, val_split, test_split = {}, {}, {}\n",
        "for relation, samples in full_dataset.items():\n",
        "    randomiser.shuffle(samples)\n",
        "    val_samples, test_samples, train_samples = np.split(samples, [\n",
        "            int(VAL_TEST_SPLIT[0] * len(samples)),\n",
        "            int((VAL_TEST_SPLIT[0] + VAL_TEST_SPLIT[1]) * len(samples))\n",
        "    ])\n",
        "    train_split.setdefault(relation, []).extend(train_samples)\n",
        "    val_split.setdefault(relation, []).extend(val_samples)\n",
        "    test_split.setdefault(relation, []).extend(test_samples)\n",
        "\n",
        "# Save new data splits\n",
        "with open(OUTPUT_DATA_SPLITS[\"train\"], \"w\") as f:\n",
        "    json.dump(train_split, f)\n",
        "with open(OUTPUT_DATA_SPLITS[\"val\"], \"w\") as f:\n",
        "    json.dump(val_split, f)\n",
        "with open(OUTPUT_DATA_SPLITS[\"test\"], \"w\") as f:\n",
        "    json.dump(test_split, f)\n",
        "\n",
        "# Print information\n",
        "for split_name, split in ([\n",
        "    (\"Train\", train_split),\n",
        "    (\"Validation\", val_split),\n",
        "    (\"Test\", test_split)\n",
        "]):\n",
        "    if ADD_NO_RELATION_SAMPLES:\n",
        "        split_lens_excl_no_relation = [\n",
        "            len(samples) for relation, samples in split.items()\n",
        "            if relation != NO_RELATION_ID\n",
        "        ]\n",
        "        print(\n",
        "            f\"{split_name} split info:\"\n",
        "            f\"\\n\\tNum. relations (excl. no relation): \"\n",
        "            f\"{len(split_lens_excl_no_relation)}\"\n",
        "            f\"\\n\\tAvg. samples per relation (excl. no relation): \"\n",
        "            f\"{int(np.mean(split_lens_excl_no_relation))}\"\n",
        "            f\"\\n\\tNum. no relation samples: \"\n",
        "            f\"{len(split.get(NO_RELATION_ID, []))}\\n\"\n",
        "        )\n",
        "    else:\n",
        "        print(\n",
        "            f\"{split_name} split info:\"\n",
        "            f\"\\n\\tNum. relations: {len(split)}\"\n",
        "            f\"\\n\\tAvg. samples per relation: \"\n",
        "            f\"{int(np.mean([len(samples) for samples in split.values()]))}\\n\"\n",
        "        )\n"
      ]
    }
  ]
}