{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUr6CrINcUfh"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Only using the Wikipedia-based data splits\n",
        "DATA_SPLITS = [\n",
        "    \"train_wiki.json\",\n",
        "    \"val_wiki.json\"\n",
        "]\n",
        "\n",
        "RANDOM_SEED = 41332\n",
        "VAL_TEST_SPLIT = [0.1, 0.1]\n",
        "\n",
        "NO_RELATION_ID = \"NA\"\n",
        "NO_RELATION_ORDER_DEPENDENT = True # If True, will generate both (a, b) and\n",
        "                                   # (b, a) samples. If false, will only\n",
        "                                   # generate one of the two.\n",
        "\n",
        "# Aggregate splits into single dataset\n",
        "with open(DATA_SPLITS[0], \"r\") as f:\n",
        "    full_dataset = json.load(f)\n",
        "for i in range(1, len(DATA_SPLITS)):\n",
        "    with open(DATA_SPLITS[i], \"r\") as f:\n",
        "        dataset = json.load(f)\n",
        "        for relation, samples in dataset.items():\n",
        "            full_dataset.setdefault(relation, [])\n",
        "            full_dataset[relation].extend(samples)\n",
        "\n",
        "# Remove entity type information (don't think this is useful to us)\n",
        "for samples in full_dataset.values():\n",
        "    for sample in samples:\n",
        "        del sample[\"h\"][1]\n",
        "        del sample[\"t\"][1]\n",
        "\n",
        "# Identify entities and relations per token sequence\n",
        "tokens_to_entities = {}\n",
        "tokens_to_relations = {}\n",
        "for samples in full_dataset.values():\n",
        "    for sample in samples:\n",
        "        tokens = tuple(sample[\"tokens\"])\n",
        "        tokens_to_entities.setdefault(tokens, {}).update({\n",
        "            sample[\"h\"][0]: sample[\"h\"],\n",
        "            sample[\"t\"][0]: sample[\"t\"]\n",
        "        })\n",
        "        tokens_to_relations.setdefault(tokens, set()).add(\n",
        "            (sample[\"h\"][0], sample[\"t\"][0])\n",
        "        )\n",
        "\n",
        "# Add no relation (NA) class to dataset\n",
        "if NO_RELATION_ORDER_DEPENDENT:\n",
        "    generator = itertools.permutations\n",
        "else:\n",
        "    generator = itertools.combinations\n",
        "for tokens, entities in tokens_to_entities.items():\n",
        "    for a, b in generator(entities.keys(), 2):\n",
        "        if (\n",
        "            (a, b) not in tokens_to_relations[tokens]\n",
        "            and (b, a) not in tokens_to_relations[tokens]\n",
        "        ):\n",
        "            full_dataset.setdefault(NO_RELATION_ID, []).append({\n",
        "                \"tokens\": list(tokens),\n",
        "                \"h\": entities[a],\n",
        "                \"t\": entities[b]\n",
        "            })\n",
        "\n",
        "# Set random seed\n",
        "randomiser = random.Random(RANDOM_SEED)\n",
        "\n",
        "# Split full dataset into new train/val/test splits\n",
        "train_split, val_split, test_split = {}, {}, {}\n",
        "for relation, samples in full_dataset.items():\n",
        "    randomiser.shuffle(samples)\n",
        "    val_samples, test_samples, train_samples = np.split(samples, [\n",
        "            int(VAL_TEST_SPLIT[0] * len(samples)),\n",
        "            int((VAL_TEST_SPLIT[0] + VAL_TEST_SPLIT[1]) * len(samples))\n",
        "    ])\n",
        "    train_split.setdefault(relation, []).extend(train_samples)\n",
        "    val_split.setdefault(relation, []).extend(val_samples)\n",
        "    test_split.setdefault(relation, []).extend(test_samples)\n",
        "\n",
        "# Save new data splits\n",
        "with open(\"train.json\", \"w\") as f:\n",
        "    json.dump(train_split, f)\n",
        "with open(\"val.json\", \"w\") as f:\n",
        "    json.dump(val_split, f)\n",
        "with open(\"test.json\", \"w\") as f:\n",
        "    json.dump(test_split, f)\n",
        "\n",
        "# Print debug information\n",
        "for split_name, split in ([\n",
        "    (\"Train\", train_split),\n",
        "    (\"Validation\", val_split),\n",
        "    (\"Test\", test_split)\n",
        "]):\n",
        "    print(\n",
        "        f\"{split_name} split info:\"\n",
        "        f\"\\n\\tNum. relations (excl. no relation): {len(split) - 1}\"\n",
        "        f\"\\n\\tAvg. samples per relation (excl. no relation): {int(np.mean([len(samples) for relation, samples in split.items() if relation != NO_RELATION_ID]))}\"\n",
        "        f\"\\n\\tNum. no relation samples: {len(split[NO_RELATION_ID])}\\n\"\n",
        "    )\n"
      ]
    }
  ]
}