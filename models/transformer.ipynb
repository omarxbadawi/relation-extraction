{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nHlWAUOJZFh"
      },
      "source": [
        "# Options (ALWAYS RUN)\n",
        "## Transformer Model\n",
        "*   **ENTITY_DELIM_TOKENS** - Entity delimiter tokens used in the dataset.\n",
        "\n",
        "## Model Training\n",
        "*   **TRAIN_DATA_PATH** - File paths of the data used for model training, if multiple paths are given then the data will be pooled.\n",
        "\n",
        "*   **VAL_DATA_PATH** - File paths of the data used for model validation, if multiple paths are given then the data will be pooled and if no paths are given then no validation will take place *(useful for final model training)*.\n",
        "\n",
        "*   **TRAINING_LOG_PATH** - File path to which training logs will be saved, if no path is given then logs won't be saved.\n",
        "\n",
        "*   **BERT_MODEL** - Name of the pre-trained BERT model to load and use as part of the relation extraction model.\n",
        "\n",
        "*   **ENTITY_TRANSFORMER_NUM_HEAD_VALS** - Hyperparameter values to iterate over for the number of heads used in the model's entity transformer. *(Note: for model testing and evaluation, the first value will be used)*\n",
        "\n",
        "*   **ENTITY_TRANSFORMER_FF_DIM_VALS** - Hyperparameter values to iterate over for the feedforward dimension used in the model's entity transformer. *(Note: for model testing and evaluation, the first value will be used)*\n",
        "\n",
        "*   **ENTITY_TRANSFORMER_NUM_LAYER_VALS** - Hyperparameter values to iterate over for the number of transformer layers used in the model's entity transformer. *(Note: for model testing and evaluation, the first value will be used)*\n",
        "\n",
        "*   **NUM_EPOCHS** - Number of epochs to use during training.\n",
        "\n",
        "*   **BATCH_SIZE** - Batch size to use during training.\n",
        "\n",
        "*   **LEARNING_RATE** - ADAM learning rate to use during training.\n",
        "\n",
        "*   **DROPOUT_RATE** - Dropout rate to use during training.\n",
        "\n",
        "*   **USE_CLASS_WEIGHTING** - Whether to use a class weighting scheme during training.\n",
        "\n",
        "*   **MODEL_SAVE_PATH** - File path to save the trained model to, this will be overwritten and hence will always contain the **LAST** model trained.\n",
        "\n",
        "## Model Evaluation\n",
        "\n",
        "*   **TEST_DATA_PATH** - File paths of the data used for model testing, if multiple paths are given then the data will be pooled.\n",
        "\n",
        "*   **TESTING_LOG_PATH** - File path to which testing logs will be saved, if no path is given then logs won't be saved.\n",
        "\n",
        "## Model Demo\n",
        "\n",
        "*   **RELATION_TO_DESCRIPTION_PATH** - File path of the relation-ID-to-description mapping file.\n",
        "\n",
        "*   **NUM_PRINTED_PREDICTIONS** - Number of predictions to print per input during the demo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yQjcum6eG4Jr"
      },
      "outputs": [],
      "source": [
        "# === OPTIONS ==================================================================\n",
        "\n",
        "# ====== TRANSFORMER MODEL ==================================================\n",
        "\n",
        "ENTITY_DELIM_TOKENS = {\n",
        "    \"h\": (\"[E1S]\", \"[E1E]\"),\n",
        "    \"t\": (\"[E2S]\", \"[E2E]\")\n",
        "}\n",
        "\n",
        "# ====== MODEL TRAINING =====================================================\n",
        "\n",
        "TRAIN_DATA_PATH = (\n",
        "    \"../dataset/train.json\",\n",
        "    \"../dataset/val.json\"\n",
        ")\n",
        "\n",
        "VAL_DATA_PATH = []\n",
        "\n",
        "TRAINING_LOG_PATH = \"logs/transformer_final_training.log\"\n",
        "\n",
        "BERT_MODEL = \"bert-base-uncased\"\n",
        "\n",
        "ENTITY_TRANSFORMER_NUM_HEAD_VALS = (2,) # (2, 4, 8)\n",
        "\n",
        "ENTITY_TRANSFORMER_FF_DIM_VALS = (256,) # (64, 128, 256)\n",
        "\n",
        "ENTITY_TRANSFORMER_NUM_LAYER_VALS = (3,) # (1, 2, 3)\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "USE_CLASS_WEIGHTING = True\n",
        "\n",
        "MODEL_SAVE_PATH = \"transformer.pt\"\n",
        "\n",
        "# ====== MODEL EVALUATION ===================================================\n",
        "\n",
        "TEST_DATA_PATH = (\n",
        "    \"../dataset/test.json\",\n",
        ")\n",
        "\n",
        "TESTING_LOG_PATH = \"logs/transformer_testing.log\"\n",
        "\n",
        "# ====== MODEL DEMO =========================================================\n",
        "\n",
        "RELATION_TO_DESCRIPTION_PATH = \"../dataset/pid2name_filtered.json\"\n",
        "\n",
        "NUM_PRINTED_PREDICTIONS = 3\n",
        "\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7306fW9Nyij"
      },
      "source": [
        "# Transformer Model (ALWAYS RUN)\n",
        "\n",
        "Our relation extraction transformer model is based off the architecture described in the paper: [Enriching Pre-trained Language Model with Entity Information for Relation Classification](https://dl.acm.org/doi/abs/10.1145/3357384.3358119).\n",
        "\n",
        "In our model, however, we have made a novel adaption by using weighted entity token averages in place of plain averages. To calculate the weights used during entity token averaging, we have inserted a new transformer encoder plus feedforward softmax layer into the architecture which takes entity token embeddings and outputs entity token weights.\n",
        "\n",
        "We have made this adaption following the assumption that each token within an entity holds varying levels of importance regarding relation extraction. E.g. in the entity phrase \"King of England\", the word \"of\" is likely to be of less importance than the word \"King\" or \"England\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VFIMNvTv7nPc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/schill/git/relation-extraction/torch/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from itertools import chain\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import (\n",
        "    pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# Relation Extraction model\n",
        "class REModel(nn.Module):\n",
        "\n",
        "    # Create BERT tokenizer and add special entity delimiter tokens\n",
        "    def create_tokenizer(bert_model, entity_delim_tokens):\n",
        "        REModel.tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
        "        REModel.tokenizer.add_special_tokens(\n",
        "            {\n",
        "                'additional_special_tokens': list(\n",
        "                    chain.from_iterable(entity_delim_tokens.values())\n",
        "                )\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Tokenization function\n",
        "    def tokenize(inputs):\n",
        "        return REModel.tokenizer(\n",
        "            inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "            return_token_type_ids=False\n",
        "        )\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            bert_model,\n",
        "            entity_delim_tokens,\n",
        "            entity_transformer_nhead,\n",
        "            entity_transformer_dim_ff,\n",
        "            entity_transformer_num_layers,\n",
        "            num_outputs,\n",
        "            dropout_rate\n",
        "    ):\n",
        "        super(REModel, self).__init__()\n",
        "\n",
        "        # Get entity delimiter token IDs\n",
        "        self.entity_delim_ids = {}\n",
        "        for entity_id, delim_tokens in entity_delim_tokens.items():\n",
        "            self.entity_delim_ids[\n",
        "                entity_id\n",
        "            ] = REModel.tokenizer.convert_tokens_to_ids(delim_tokens)\n",
        "\n",
        "        # Create BERT model\n",
        "        self.bert = BertModel.from_pretrained(bert_model)\n",
        "        self.bert.resize_token_embeddings(len(REModel.tokenizer))\n",
        "        bert_hidden_size = self.bert.config.hidden_size\n",
        "\n",
        "        # Create feedforward layer for [CLS] token\n",
        "        self.cls_feedforward = nn.Linear(\n",
        "            bert_hidden_size,\n",
        "            bert_hidden_size\n",
        "        )\n",
        "\n",
        "        # Create transformer encoder for calculating entity weights\n",
        "        entity_transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=bert_hidden_size,\n",
        "            nhead=entity_transformer_nhead,\n",
        "            dim_feedforward=entity_transformer_dim_ff,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.entity_transformer = nn.TransformerEncoder(\n",
        "            entity_transformer_layer,\n",
        "            num_layers=entity_transformer_num_layers,\n",
        "        )\n",
        "\n",
        "        # Create feedforward layer for calculating entity weights\n",
        "        self.entity_weight_feedforward = nn.Linear(bert_hidden_size, 1)\n",
        "\n",
        "        # Create feedforward layer for weighted average entity embeddings\n",
        "        self.entity_feedforward = nn.Linear(\n",
        "            bert_hidden_size,\n",
        "            bert_hidden_size\n",
        "        )\n",
        "\n",
        "        # Create feedforward layer for final relation embedding\n",
        "        self.relation_feedforward = nn.Linear(\n",
        "            bert_hidden_size * (1 + len(entity_delim_tokens)),\n",
        "            num_outputs\n",
        "        )\n",
        "\n",
        "        # Create dropout layer\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        # Pass inputs through BERT model\n",
        "        bert_outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        ).last_hidden_state\n",
        "\n",
        "        # Pass [CLS] token embedding through tanh + feedforward layer\n",
        "        cls_embedding = self.cls_feedforward(\n",
        "            torch.tanh(self.dropout(bert_outputs[:, 0, :]))\n",
        "        )\n",
        "\n",
        "        # Calculate weighted average entity embeddings and concatenate with\n",
        "        # [CLS] token embedding to create final relation embedding\n",
        "        #\n",
        "        # TODO : Could probably fully vectorise this across entity types to\n",
        "        # remove the for loop here. I think it'd be pretty complicated to do\n",
        "        # this though and, since our usecase only requires 2 entity types, I\n",
        "        # don't think the performance boost would be too big anyway.\n",
        "        #\n",
        "        relation_embedding = cls_embedding\n",
        "        for start_id, end_id in self.entity_delim_ids.values():\n",
        "\n",
        "            # Find entity delimiter positions\n",
        "            start_poses = input_ids == start_id\n",
        "            end_poses = input_ids == end_id\n",
        "\n",
        "            # Get entity mask\n",
        "            start_cumsum = torch.cumsum(start_poses, dim=1)\n",
        "            end_cumsum = torch.cumsum(end_poses, dim=1)\n",
        "            entity_mask = (\n",
        "                (start_cumsum > end_cumsum) & (start_cumsum > 0) & ~start_poses\n",
        "            )\n",
        "\n",
        "            # Extract entity token embeddings\n",
        "            entity_tokens = torch.split(\n",
        "                bert_outputs[entity_mask],\n",
        "                entity_mask.sum(dim=1).tolist()\n",
        "            )\n",
        "\n",
        "            # Normalise token embedding counts across inputs using padding\n",
        "            padded_entity_tokens = pad_sequence(\n",
        "                entity_tokens,\n",
        "                batch_first=True,\n",
        "                padding_value=0\n",
        "            )\n",
        "\n",
        "            # Create attention mask for padded entity token embeddings\n",
        "            entity_token_attention = (\n",
        "                (padded_entity_tokens == 0).all(dim=-1)\n",
        "            ).float()\n",
        "\n",
        "            # Pass entity token embeddings through entity transformer encoder\n",
        "            entity_transformer_outputs = self.entity_transformer(\n",
        "                padded_entity_tokens,\n",
        "                src_key_padding_mask=entity_token_attention\n",
        "            )\n",
        "\n",
        "            # Pack all entity token embedding sequences together, ignorning\n",
        "            # padding tokens\n",
        "            packed_entity_transformer_outputs = pack_padded_sequence(\n",
        "                entity_transformer_outputs,\n",
        "                (entity_token_attention == 0).sum(dim=1).to(\"cpu\"),\n",
        "                batch_first=True,\n",
        "                enforce_sorted=False\n",
        "            )\n",
        "\n",
        "            # Pass packed entity token embeddings through feedforward layer\n",
        "            packed_entity_embedding_weights = self.entity_weight_feedforward(\n",
        "                packed_entity_transformer_outputs.data\n",
        "            )\n",
        "\n",
        "            # Unpack entity embedding weights to restore per input data\n",
        "            batch_size = entity_transformer_outputs.shape[0]\n",
        "            entity_token_sequence_len = entity_transformer_outputs.shape[1]\n",
        "            entity_embedding_weights = torch.full(\n",
        "                (batch_size * entity_token_sequence_len,),\n",
        "                float('-inf'),\n",
        "                device=input_ids.device\n",
        "            )\n",
        "            entity_embedding_weights[\n",
        "                (entity_token_attention.flatten() == 0).nonzero().squeeze()\n",
        "            ] = packed_entity_embedding_weights.flatten()\n",
        "            entity_embedding_weights = entity_embedding_weights.view(\n",
        "                (batch_size, entity_token_sequence_len)\n",
        "            )\n",
        "\n",
        "            # Softmax entity embedding weights over each input\n",
        "            entity_embedding_weights = nn.functional.softmax(\n",
        "                entity_embedding_weights, dim=-1\n",
        "            )\n",
        "\n",
        "            # Calculate weighted average entity embedding\n",
        "            weighted_entity_embedding = (\n",
        "                padded_entity_tokens * entity_embedding_weights.unsqueeze(-1)\n",
        "            ).sum(dim=1)\n",
        "\n",
        "            # Pass weighted average entity embedding through tanh + feedforward\n",
        "            # layer\n",
        "            avg_entity_output = self.entity_feedforward(\n",
        "                torch.tanh(self.dropout(weighted_entity_embedding))\n",
        "            )\n",
        "\n",
        "            # Concatenate average entity output with final relation embedding\n",
        "            relation_embedding = torch.cat(\n",
        "                [relation_embedding, avg_entity_output], dim=-1\n",
        "            )\n",
        "\n",
        "        # Pass final relation embedding through feedforward layer\n",
        "        output = self.relation_feedforward(self.dropout(relation_embedding))\n",
        "\n",
        "        return output\n",
        "\n",
        "# Define PyTorch dataset class for dataloader\n",
        "class REDataset(Dataset):\n",
        "    def __init__(self, relation_id_map, data, tokenizer):\n",
        "\n",
        "        # Get list of tokens and labels from data\n",
        "        tokens = []\n",
        "        self.labels = []\n",
        "        for relation, samples in data.items():\n",
        "            tokens += [sample[\"tokens\"] for sample in samples]\n",
        "            self.labels += [relation_id_map[relation]] * len(samples)\n",
        "\n",
        "        # Tokenize sentences\n",
        "        tokenized = tokenizer(tokens)\n",
        "        self.input_ids = tokenized[\"input_ids\"]\n",
        "        self.attention_mask = tokenized[\"attention_mask\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.input_ids[idx],\n",
        "            self.attention_mask[idx],\n",
        "            torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtCcrO6nPRAB"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu6rNwhsFhej",
        "outputId": "a2a807eb-25a7-4738-e99d-3d7e75106a8d",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num. Heads: 2, F.F. Dim.: 256, Num. Layers: 3\n",
            "\n",
            "\tEpoch: 1/3\n",
            "\tSample: 54108/54108, Loss: 0.4776819944381714, Time Rem.: 0s          \n",
            "\tTraining Loss: 1.0128691790009712\n",
            "\n",
            "\tEpoch: 2/3\n",
            "\tSample: 54108/54108, Loss: 0.401192843914032, Time Rem.: 0s          \n",
            "\tTraining Loss: 0.33581728160398944\n",
            "\n",
            "\tEpoch: 3/3\n",
            "\tSample: 54108/54108, Loss: 0.33058929443359375, Time Rem.: 0s          \n",
            "\tTraining Loss: 0.2132370227318321\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load training and validation data\n",
        "train_data = {}\n",
        "val_data = {}\n",
        "for data_dict, data_paths in (\n",
        "    (train_data, TRAIN_DATA_PATH),\n",
        "    (val_data, VAL_DATA_PATH)\n",
        "):\n",
        "    for data_path in data_paths:\n",
        "        with open(data_path) as f:\n",
        "            new_data = json.load(f)\n",
        "            for relation, samples in new_data.items():\n",
        "                data_dict.setdefault(relation, []).extend(samples)\n",
        "\n",
        "# Create relation to ID mapping\n",
        "relation_id_map = {}\n",
        "for relation in sorted(list(train_data.keys())):\n",
        "    relation_id_map[relation] = len(relation_id_map)\n",
        "\n",
        "# Create REModel tokenizer\n",
        "REModel.create_tokenizer(BERT_MODEL, ENTITY_DELIM_TOKENS)\n",
        "\n",
        "# Create training and validation datasets\n",
        "train_dataset = REDataset(relation_id_map, train_data, REModel.tokenize)\n",
        "if len(val_data) > 0:\n",
        "    val_dataset = REDataset(relation_id_map, val_data, REModel.tokenize)\n",
        "else:\n",
        "    val_dataset = None\n",
        "del train_data, val_data\n",
        "\n",
        "# Create training and validation dataloaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "if val_dataset:\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Get device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Calculate class weights if needed\n",
        "class_weights = None\n",
        "if USE_CLASS_WEIGHTING:\n",
        "    class_counts = torch.bincount(torch.tensor(train_dataset.labels))\n",
        "    class_weights = (\n",
        "        len(train_dataset.labels) / (len(class_counts) * class_counts)\n",
        "    ).float().to(device)\n",
        "\n",
        "# Reset log file\n",
        "if TRAINING_LOG_PATH:\n",
        "    open(TRAINING_LOG_PATH, \"w+\").close()\n",
        "\n",
        "# Loop over all required hyperparameter combinations\n",
        "for num_head in ENTITY_TRANSFORMER_NUM_HEAD_VALS:\n",
        "    for ff_dim in ENTITY_TRANSFORMER_FF_DIM_VALS:\n",
        "        for num_layer in ENTITY_TRANSFORMER_NUM_LAYER_VALS:\n",
        "            log_text = \"\"\n",
        "\n",
        "            # Log model hyperparameters\n",
        "            text = (\n",
        "                f\"Num. Heads: {num_head}, \"\n",
        "                f\"F.F. Dim.: {ff_dim}, \"\n",
        "                f\"Num. Layers: {num_layer}\\n\"\n",
        "            )\n",
        "            print(text)\n",
        "            log_text += f\"{text}\\n\"\n",
        "\n",
        "            # Create model\n",
        "            model = REModel(\n",
        "                BERT_MODEL,\n",
        "                ENTITY_DELIM_TOKENS,\n",
        "                num_head,\n",
        "                ff_dim,\n",
        "                num_layer,\n",
        "                len(relation_id_map),\n",
        "                DROPOUT_RATE\n",
        "            )\n",
        "            model.to(device)\n",
        "\n",
        "            # Create loss function and optimizer\n",
        "            loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "            # Train model\n",
        "            model.train()\n",
        "            prev_val_loss = float(\"inf\")\n",
        "            for epoch in range(NUM_EPOCHS):\n",
        "                train_loss = 0\n",
        "                curr_sample = 0\n",
        "                start_time = time.time()\n",
        "\n",
        "                # Log current epoch\n",
        "                text = (f\"\\tEpoch: {epoch + 1}/{NUM_EPOCHS}\")\n",
        "                print(text)\n",
        "                log_text += f\"{text}\"\n",
        "\n",
        "                # Loop over all batches\n",
        "                for input_ids, attention_mask, label in train_dataloader:\n",
        "                    input_ids, attention_mask, label = (\n",
        "                        input_ids.to(device),\n",
        "                        attention_mask.to(device),\n",
        "                        label.to(device)\n",
        "                    )\n",
        "\n",
        "                    # Get model output\n",
        "                    output = model(input_ids, attention_mask)\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss = loss_func(output, label)\n",
        "                    train_loss += loss.item()\n",
        "\n",
        "                    # Optimize model\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # Log batch information\n",
        "                    curr_sample += len(label)\n",
        "                    time_per_sample = (time.time() - start_time) / curr_sample\n",
        "                    text = (\n",
        "                        f\"\\tSample: {curr_sample}/{len(train_dataset)}, \"\n",
        "                        f\"Loss: {loss.item()}, \"\n",
        "                        f\"Time Rem.: \"\n",
        "                        f\"{round(time_per_sample * (len(train_dataset) - curr_sample))}s\"\n",
        "                    )\n",
        "                    print(f\"\\r{text}\", end=\" \"*10)\n",
        "                    log_text += f\"\\n{text}\"\n",
        "\n",
        "                # Log epoch training information\n",
        "                text = (\n",
        "                    f\"\\n\\tTraining Loss: {train_loss / len(train_dataloader)}\" +\n",
        "                    (\"\\n\" if not val_dataset else \"\")\n",
        "                )\n",
        "                print(text)\n",
        "                log_text += text\n",
        "\n",
        "                # Calculate validation loss\n",
        "                if val_dataset:\n",
        "                    model.eval()\n",
        "                    val_loss = 0\n",
        "                    with torch.no_grad():\n",
        "                        for input_ids, attention_mask, label in val_dataloader:\n",
        "                            input_ids, attention_mask, label = (\n",
        "                                input_ids.to(device),\n",
        "                                attention_mask.to(device),\n",
        "                                label.to(device)\n",
        "                            )\n",
        "\n",
        "                            # Get model output\n",
        "                            output = model(input_ids, attention_mask)\n",
        "\n",
        "                            # Calculate loss\n",
        "                            loss = loss_func(output, label)\n",
        "                            val_loss += loss.item()\n",
        "\n",
        "                    # Log epoch validation information\n",
        "                    text = (\n",
        "                        f\"\\tValidation Loss: {val_loss / len(val_dataloader)}\\n\"\n",
        "                    )\n",
        "                    print(text)\n",
        "                    log_text += f\"\\n{text}\\n\"\n",
        "\n",
        "                    model.train()\n",
        "\n",
        "            # Write log to file\n",
        "            if TRAINING_LOG_PATH:\n",
        "                with open(TRAINING_LOG_PATH, \"a\") as f:\n",
        "                    f.write(log_text)\n",
        "\n",
        "# Save model weights to file\n",
        "if MODEL_SAVE_PATH:\n",
        "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVbJ5qSMPTst"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6kjEGVLG4Jy",
        "outputId": "ccc3b6f4-f5b6-4f0d-de47-206a94a5c521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample: 6012/6012, Time Rem.: 0s          \n",
            "\n",
            "Accuracy: 0.8835148811340332\n",
            "F1 Score: 0.8724994659423828\n",
            "Precision: 0.8668336868286133\n",
            "Recall: 0.8835148811340332\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics -q\n",
        "\n",
        "import json\n",
        "import time\n",
        "from torchmetrics import Accuracy, F1Score, Precision, Recall\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load testing data\n",
        "test_data = {}\n",
        "for data_path in TEST_DATA_PATH:\n",
        "    with open(data_path) as f:\n",
        "        new_data = json.load(f)\n",
        "        for relation, samples in new_data.items():\n",
        "            test_data.setdefault(relation, []).extend(samples)\n",
        "\n",
        "# Create relation to ID mapping\n",
        "relation_id_map = {}\n",
        "for relation in sorted(list(test_data.keys())):\n",
        "    relation_id_map[relation] = len(relation_id_map)\n",
        "\n",
        "# Create REModel tokenizer\n",
        "REModel.create_tokenizer(BERT_MODEL, ENTITY_DELIM_TOKENS)\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = REDataset(relation_id_map, test_data, REModel.tokenize)\n",
        "del test_data\n",
        "\n",
        "# Create test dataloaders\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Get device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model for evaluation\n",
        "model = REModel(\n",
        "    BERT_MODEL,\n",
        "    ENTITY_DELIM_TOKENS,\n",
        "    ENTITY_TRANSFORMER_NUM_HEAD_VALS[0],\n",
        "    ENTITY_TRANSFORMER_FF_DIM_VALS[0],\n",
        "    ENTITY_TRANSFORMER_NUM_LAYER_VALS[0],\n",
        "    len(relation_id_map),\n",
        "    DROPOUT_RATE\n",
        ")\n",
        "model.load_state_dict(torch.load(\n",
        "    MODEL_SAVE_PATH, weights_only=True\n",
        "), strict=False)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Evaluate model using test data\n",
        "predicted = []\n",
        "actual = []\n",
        "curr_sample = 0\n",
        "start_time = time.time()\n",
        "log_text = \"\"\n",
        "for input_ids, attention_mask, label in test_dataloader:\n",
        "    input_ids, attention_mask = (\n",
        "        input_ids.to(device),\n",
        "        attention_mask.to(device)\n",
        "    )\n",
        "\n",
        "    # Get model output\n",
        "    output = model(input_ids, attention_mask)\n",
        "\n",
        "    # Get predicted relation ID\n",
        "    output = nn.functional.softmax(output, dim=-1)\n",
        "    predicted.extend(torch.argmax(output, dim=-1).tolist())\n",
        "\n",
        "    # Get actual relation ID\n",
        "    actual.extend(label.tolist())\n",
        "\n",
        "    # Log batch information\n",
        "    curr_sample += len(label)\n",
        "    time_per_sample = (time.time() - start_time) / curr_sample\n",
        "    text = (\n",
        "        f\"Sample: {curr_sample}/{len(test_dataset)}, \"\n",
        "        f\"Time Rem.: \"\n",
        "        f\"{round(time_per_sample * (len(test_dataset) - curr_sample))}s\"\n",
        "    )\n",
        "    print(f\"\\r{text}\", end=\" \"*10)\n",
        "    log_text += f\"{text}\\n\"\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "print(\"\\n\")\n",
        "for metric_name, metric_func in (\n",
        "    (\"Accuracy\", Accuracy(\n",
        "            task=\"multiclass\",\n",
        "            num_classes=len(relation_id_map),\n",
        "            average=\"macro\"\n",
        "        )\n",
        "    ),\n",
        "    (\"F1 Score\", F1Score(\n",
        "            task=\"multiclass\",\n",
        "            num_classes=len(relation_id_map),\n",
        "            average=\"macro\"\n",
        "        )\n",
        "    ),\n",
        "    (\"Precision\", Precision(\n",
        "            task=\"multiclass\",\n",
        "            num_classes=len(relation_id_map),\n",
        "            average=\"macro\"\n",
        "        )\n",
        "    ),\n",
        "    (\"Recall\", Recall(\n",
        "            task=\"multiclass\",\n",
        "            num_classes=len(relation_id_map),\n",
        "            average=\"macro\"\n",
        "        )\n",
        "    )\n",
        "):\n",
        "\n",
        "    # Calculate metric\n",
        "    result = metric_func(torch.tensor(predicted), torch.tensor(actual))\n",
        "\n",
        "    # Log metric information\n",
        "    text = f\"{metric_name}: {result.item()}\"\n",
        "    print(f\"{text}\")\n",
        "    log_text += f\"\\n{text}\"\n",
        "\n",
        "# Save log to file\n",
        "with open(TESTING_LOG_PATH, \"w+\") as f:\n",
        "    f.write(log_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9trEKqXP4dp"
      },
      "source": [
        "# Model Demo\n",
        "\n",
        "**Inputs must include the entity delimiters \"[E1S]\" and \"[E1E]\" around occurences of the first entity, and \"[E2S]\" and \"[E2E]\" around occurences of the second entity.**\n",
        "\n",
        "**E.g: \"[E1S]Charles[E1E] is the [E2S]King of England[E2E].\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u3NbLgLG4Jz",
        "outputId": "63231acb-2fe3-44b6-c938-0394ef8bc186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from transformer.pt\n",
            "\n",
            "Input: [E1S]Charles[E1E] is the [E2S]King of England[E2E].\n",
            "\n",
            "93.73% - position held (subject currently or formerly holds the object position or public office)\n",
            "\n",
            "5.96% - occupation (occupation of a person; see also \"field of work\" (Property:P101), \"position held\" (Property:P39))\n",
            "\n",
            "0.04% - military rank (military rank achieved by a person (should usually have a \"start time\" qualifier), or military rank associated with a position)\n",
            "\n",
            "Input: \n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Load relation to description mapping\n",
        "with open(RELATION_TO_DESCRIPTION_PATH) as f:\n",
        "    relation_to_desc = json.load(f)\n",
        "\n",
        "# Create ID to relation mapping\n",
        "relation_id_map = {}\n",
        "for relation in sorted(list(relation_to_desc.keys())):\n",
        "    relation_id_map[relation] = len(relation_id_map)\n",
        "id_relation_map = {iid: rel for rel, iid in relation_id_map.items()}\n",
        "\n",
        "# Get device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create REModel tokenizer\n",
        "REModel.create_tokenizer(BERT_MODEL, ENTITY_DELIM_TOKENS)\n",
        "\n",
        "# Load model for demo\n",
        "print(f\"Loading model from {MODEL_SAVE_PATH}\")\n",
        "model = REModel(\n",
        "    BERT_MODEL,\n",
        "    ENTITY_DELIM_TOKENS,\n",
        "    ENTITY_TRANSFORMER_NUM_HEAD_VALS[0],\n",
        "    ENTITY_TRANSFORMER_FF_DIM_VALS[0],\n",
        "    ENTITY_TRANSFORMER_NUM_LAYER_VALS[0],\n",
        "    len(relation_to_desc),\n",
        "    DROPOUT_RATE\n",
        ")\n",
        "model.load_state_dict(torch.load(\n",
        "    MODEL_SAVE_PATH, weights_only=True\n",
        "), strict=False)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Demo loop\n",
        "while True:\n",
        "\n",
        "    # Get input\n",
        "    inp = input(\"\\nInput: \")\n",
        "    if inp == \"\":\n",
        "        break\n",
        "\n",
        "    # Convert input to required model input format\n",
        "    inp = REModel.tokenizer(inp)\n",
        "    input_ids = torch.tensor(inp[\"input_ids\"]).unsqueeze(0).to(device)\n",
        "    attention_mask = torch.tensor(inp[\"attention_mask\"]).unsqueeze(0).to(device)\n",
        "\n",
        "    # Get model output\n",
        "    output = model(input_ids, attention_mask)\n",
        "    output = nn.functional.softmax(output, dim=-1)\n",
        "\n",
        "    # Get the top 3 relation IDs and probabilities\n",
        "    top_probs, top_ids = torch.topk(output, k=NUM_PRINTED_PREDICTIONS)\n",
        "    top_probs *= 100\n",
        "\n",
        "    # Print results\n",
        "    for i in range(NUM_PRINTED_PREDICTIONS):\n",
        "        desc = relation_to_desc[id_relation_map[top_ids[0][i].item()]]\n",
        "        print(\n",
        "            f\"\\n{top_probs[0][i].item():.2f}% - \"\n",
        "            f\"{desc[0]} ({desc[1]})\"\n",
        "        )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
