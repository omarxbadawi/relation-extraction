{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy tensorflow keras-tuner scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 81 training relations, 81 validation, 81 test.\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir):\n",
    "    train_file = os.path.join(data_dir, \"train.json\")\n",
    "    val_file = os.path.join(data_dir, \"val.json\")\n",
    "    test_file = os.path.join(data_dir, \"test.json\")\n",
    "\n",
    "    with open(train_file, \"r\") as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(val_file, \"r\") as f:\n",
    "        val_data = json.load(f)\n",
    "    with open(test_file, \"r\") as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "data_dir = \"../dataset\"  # Move out of 'models' and into 'dataset'\n",
    "train_data, val_data, test_data = load_data(data_dir)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training relations, {len(val_data)} validation, {len(test_data)} test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: Employed by Australian National Airways (ANA) after leaving the Air Force, Lukis become airfield manager at [E1S] Essendon [E1E], [E2S] Melbourne [E2E].\n",
      "Label dictionary: {'P931': 0, 'P4552': 1, 'P140': 2, 'P1923': 3, 'P150': 4, 'P6': 5, 'P27': 6, 'P449': 7, 'P1435': 8, 'P175': 9, 'P1344': 10, 'P39': 11, 'P527': 12, 'P740': 13, 'P706': 14, 'P84': 15, 'P495': 16, 'P123': 17, 'P57': 18, 'P22': 19, 'P178': 20, 'P241': 21, 'P403': 22, 'P1411': 23, 'P135': 24, 'P991': 25, 'P156': 26, 'P176': 27, 'P31': 28, 'P1877': 29, 'P102': 30, 'P1408': 31, 'P159': 32, 'P3373': 33, 'P1303': 34, 'P17': 35, 'P106': 36, 'P551': 37, 'P937': 38, 'P355': 39, 'P710': 40, 'P137': 41, 'P674': 42, 'P466': 43, 'P136': 44, 'P306': 45, 'P127': 46, 'P400': 47, 'P974': 48, 'P1346': 49, 'P460': 50, 'P86': 51, 'P118': 52, 'P264': 53, 'P750': 54, 'P58': 55, 'P3450': 56, 'P105': 57, 'P276': 58, 'P101': 59, 'P407': 60, 'P1001': 61, 'P800': 62, 'P131': 63, 'P177': 64, 'P364': 65, 'P2094': 66, 'P361': 67, 'P641': 68, 'P59': 69, 'P413': 70, 'P206': 71, 'P412': 72, 'P155': 73, 'P26': 74, 'P410': 75, 'P25': 76, 'P463': 77, 'P40': 78, 'P921': 79, 'NA': 80}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(data):\n",
    "    texts, labels = [], []\n",
    "    label_dict = {}\n",
    "    label_index = 0\n",
    "\n",
    "    for relation, samples in data.items():\n",
    "        if relation not in label_dict:\n",
    "            label_dict[relation] = label_index\n",
    "            label_index += 1\n",
    "\n",
    "        for sample in samples:\n",
    "            texts.append(sample[\"tokens\"])\n",
    "            labels.append(label_dict[relation])\n",
    "\n",
    "    return texts, np.array(labels), label_dict\n",
    "\n",
    "\n",
    "train_texts, train_labels, label_dict = preprocess_data(train_data)\n",
    "val_texts, val_labels, _ = preprocess_data(val_data)\n",
    "test_texts, test_labels, _ = preprocess_data(test_data)\n",
    "\n",
    "print(f\"Sample text: {train_texts[0]}\")\n",
    "print(f\"Label dictionary: {label_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 115836\n",
      "Sample tokenized sequence: [3682   14  347   45 3242    1   42 1862    2  181 6594    1  602 3801\n",
      "  803   25    4    1   19    3 1283   18    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_SEQ_LENGTH = 100\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\", filters=\"\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "train_sequences = pad_sequences(tokenizer.texts_to_sequences(\n",
    "    train_texts), maxlen=MAX_SEQ_LENGTH, padding=\"post\")\n",
    "val_sequences = pad_sequences(tokenizer.texts_to_sequences(\n",
    "    val_texts), maxlen=MAX_SEQ_LENGTH, padding=\"post\")\n",
    "test_sequences = pad_sequences(tokenizer.texts_to_sequences(\n",
    "    test_texts), maxlen=MAX_SEQ_LENGTH, padding=\"post\")\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Sample tokenized sequence: {train_sequences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    inputs = Input(shape=(MAX_SEQ_LENGTH,))\n",
    "\n",
    "    # Embedding Layer\n",
    "    embedding = Embedding(\n",
    "        input_dim=MAX_VOCAB_SIZE,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "    )(inputs)\n",
    "\n",
    "    # First Conv1D Layer\n",
    "    conv1 = Conv1D(\n",
    "        filters=hp.Int(\"filters_1\", min_value=64, max_value=256, step=64),\n",
    "        kernel_size=hp.Choice(\"kernel_size_1\", values=[3, 5, 7]),\n",
    "        activation='relu'\n",
    "    )(embedding)\n",
    "\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pooled1 = GlobalMaxPooling1D()(conv1)\n",
    "\n",
    "    # Second Conv1D Layer\n",
    "    conv2 = Conv1D(\n",
    "        filters=hp.Int(\"filters_2\", min_value=64, max_value=256, step=64),\n",
    "        kernel_size=hp.Choice(\"kernel_size_2\", values=[3, 5, 7]),\n",
    "        activation='relu'\n",
    "    )(embedding)  # Use embedding as input again\n",
    "\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pooled2 = GlobalMaxPooling1D()(conv2)\n",
    "\n",
    "    # Concatenate both pooled layers\n",
    "    merged = tf.keras.layers.Concatenate()([pooled1, pooled2])\n",
    "\n",
    "    dense = Dense(hp.Int(\"dense_units\", min_value=32,\n",
    "                  max_value=128, step=32), activation='relu')(merged)\n",
    "\n",
    "    dropout = Dropout(hp.Float(\"dropout_rate\", min_value=0.2,\n",
    "                      max_value=0.5, step=0.1))(dense)\n",
    "\n",
    "    outputs = Dense(len(label_dict), activation='softmax')(dropout)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=hp.Choice(\n",
    "            \"learning_rate\", values=[1e-2, 1e-3, 1e-4])),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Train Sequences Shape: {train_sequences.shape}\")  # DEBUG\n",
    "# print(f\"Train Labels Shape: {train_labels.shape}\")  # DEBUG\n",
    "\n",
    "# print(f\"Validation Sequences Shape: {val_sequences.shape}\")  # DEBUG\n",
    "# print(f\"Validation Labels Shape: {val_labels.shape}\")  # DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from cnn/hyperparameter_tuning/cnn_relation_extraction/tuner0.json\n",
      "Best hyperparameters: {'filters_1': 256, 'kernel_size_1': 3, 'filters_2': 64, 'kernel_size_2': 5, 'dense_units': 128, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"cnn/hyperparameter_tuning\",\n",
    "    project_name=\"cnn_relation_extraction\"\n",
    ")\n",
    "\n",
    "tuner.search(train_sequences, train_labels, validation_data=(\n",
    "    val_sequences, val_labels), epochs=10, batch_size=32)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best hyperparameters: {best_hps.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - accuracy: 0.0821 - loss: 4.2498 - val_accuracy: 0.2959 - val_loss: 3.0123\n",
      "Epoch 2/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 20ms/step - accuracy: 0.3448 - loss: 2.7904 - val_accuracy: 0.5098 - val_loss: 1.9991\n",
      "Epoch 3/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 21ms/step - accuracy: 0.5267 - loss: 1.8999 - val_accuracy: 0.5970 - val_loss: 1.5982\n",
      "Epoch 4/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 20ms/step - accuracy: 0.6321 - loss: 1.4285 - val_accuracy: 0.6266 - val_loss: 1.3843\n",
      "Epoch 5/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 20ms/step - accuracy: 0.7027 - loss: 1.1212 - val_accuracy: 0.6545 - val_loss: 1.2634\n",
      "Epoch 6/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 19ms/step - accuracy: 0.7630 - loss: 0.8917 - val_accuracy: 0.6667 - val_loss: 1.1874\n",
      "Epoch 7/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 20ms/step - accuracy: 0.8074 - loss: 0.7141 - val_accuracy: 0.6796 - val_loss: 1.1498\n",
      "Epoch 8/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 21ms/step - accuracy: 0.8498 - loss: 0.5621 - val_accuracy: 0.6816 - val_loss: 1.1304\n",
      "Epoch 9/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 20ms/step - accuracy: 0.8796 - loss: 0.4538 - val_accuracy: 0.6826 - val_loss: 1.1302\n",
      "Epoch 10/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 20ms/step - accuracy: 0.9033 - loss: 0.3697 - val_accuracy: 0.6880 - val_loss: 1.1392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x31daf8e90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "best_model.fit(\n",
    "    train_sequences, train_labels,\n",
    "    validation_data=(val_sequences, val_labels),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Accuracy: 0.6887890697\n",
      "F1 Score: 0.6974113804\n",
      "Precision: 0.7084025080\n",
      "Recall: 0.6931490899\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Get model predictions\n",
    "y_pred_probs = best_model.predict(test_sequences)\n",
    "y_pred = y_pred_probs.argmax(axis=1)  # Convert probabilities to class indices\n",
    "\n",
    "# Compute accuracy\n",
    "test_loss, test_acc = best_model.evaluate(\n",
    "    test_sequences, test_labels, verbose=0)\n",
    "\n",
    "# Compute precision, recall, and F1-score (macro-averaged across all classes)\n",
    "precision = precision_score(test_labels, y_pred, average=\"macro\")\n",
    "recall = recall_score(test_labels, y_pred, average=\"macro\")\n",
    "f1 = f1_score(test_labels, y_pred, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy: {test_acc:.10f}\")\n",
    "print(f\"F1 Score: {f1:.10f}\")\n",
    "print(f\"Precision: {precision:.10f}\")\n",
    "print(f\"Recall: {recall:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model and tokenizer saved successfully in 'models/' folder.\n"
     ]
    }
   ],
   "source": [
    "models_dir = \"cnn\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "best_model.save(os.path.join(models_dir, \"cnn_model.h5\"))\n",
    "\n",
    "with open(os.path.join(models_dir, \"cnn_tokenizer.json\"), \"w\") as f:\n",
    "    json.dump(tokenizer.word_index, f)\n",
    "\n",
    "print(\"Best model and tokenizer saved successfully in 'models/' folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
