{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy tensorflow keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import keras_tuner as kt  # For hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 81 training relations, 81 validation, 81 test.\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir):\n",
    "    train_file = os.path.join(data_dir, \"train.json\")\n",
    "    val_file = os.path.join(data_dir, \"val.json\")\n",
    "    test_file = os.path.join(data_dir, \"test.json\")\n",
    "\n",
    "    with open(train_file, \"r\") as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(val_file, \"r\") as f:\n",
    "        val_data = json.load(f)\n",
    "    with open(test_file, \"r\") as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "data_dir = \"../dataset\"  # Move out of 'models' and into 'dataset'\n",
    "train_data, val_data, test_data = load_data(data_dir)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training relations, {len(val_data)} validation, {len(test_data)} test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: Employed by Australian National Airways (ANA) after leaving the Air Force, Lukis become airfield manager at [E1S] Essendon [E1E], [E2S] Melbourne [E2E].\n",
      "Label dictionary: {'P931': 0, 'P4552': 1, 'P140': 2, 'P1923': 3, 'P150': 4, 'P6': 5, 'P27': 6, 'P449': 7, 'P1435': 8, 'P175': 9, 'P1344': 10, 'P39': 11, 'P527': 12, 'P740': 13, 'P706': 14, 'P84': 15, 'P495': 16, 'P123': 17, 'P57': 18, 'P22': 19, 'P178': 20, 'P241': 21, 'P403': 22, 'P1411': 23, 'P135': 24, 'P991': 25, 'P156': 26, 'P176': 27, 'P31': 28, 'P1877': 29, 'P102': 30, 'P1408': 31, 'P159': 32, 'P3373': 33, 'P1303': 34, 'P17': 35, 'P106': 36, 'P551': 37, 'P937': 38, 'P355': 39, 'P710': 40, 'P137': 41, 'P674': 42, 'P466': 43, 'P136': 44, 'P306': 45, 'P127': 46, 'P400': 47, 'P974': 48, 'P1346': 49, 'P460': 50, 'P86': 51, 'P118': 52, 'P264': 53, 'P750': 54, 'P58': 55, 'P3450': 56, 'P105': 57, 'P276': 58, 'P101': 59, 'P407': 60, 'P1001': 61, 'P800': 62, 'P131': 63, 'P177': 64, 'P364': 65, 'P2094': 66, 'P361': 67, 'P641': 68, 'P59': 69, 'P413': 70, 'P206': 71, 'P412': 72, 'P155': 73, 'P26': 74, 'P410': 75, 'P25': 76, 'P463': 77, 'P40': 78, 'P921': 79, 'NA': 80}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(data):\n",
    "    texts, labels = [], []\n",
    "    label_dict = {}\n",
    "    label_index = 0\n",
    "\n",
    "    for relation, samples in data.items():\n",
    "        if relation not in label_dict:\n",
    "            label_dict[relation] = label_index\n",
    "            label_index += 1\n",
    "\n",
    "        for sample in samples:\n",
    "            # No need to join, it's already a string\n",
    "            texts.append(sample[\"tokens\"])\n",
    "            labels.append(label_dict[relation])\n",
    "\n",
    "    return texts, np.array(labels), label_dict\n",
    "\n",
    "\n",
    "train_texts, train_labels, label_dict = preprocess_data(train_data)\n",
    "val_texts, val_labels, _ = preprocess_data(val_data)\n",
    "test_texts, test_labels, _ = preprocess_data(test_data)\n",
    "\n",
    "print(f\"Sample text: {train_texts[0]}\")\n",
    "print(f\"Label dictionary: {label_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 115836\n",
      "Sample tokenized sequence: [3682   14  347   45 3242    1   42 1862    2  181 6594    1  602 3801\n",
      "  803   25    4    1   19    3 1283   18    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_SEQ_LENGTH = 100\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\", filters=\"\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "train_sequences = pad_sequences(tokenizer.texts_to_sequences(\n",
    "    train_texts), maxlen=MAX_SEQ_LENGTH, padding=\"post\")\n",
    "val_sequences = pad_sequences(tokenizer.texts_to_sequences(\n",
    "    val_texts), maxlen=MAX_SEQ_LENGTH, padding=\"post\")\n",
    "test_sequences = pad_sequences(tokenizer.texts_to_sequences(\n",
    "    test_texts), maxlen=MAX_SEQ_LENGTH, padding=\"post\")\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Sample tokenized sequence: {train_sequences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    inputs = Input(shape=(MAX_SEQ_LENGTH,))\n",
    "\n",
    "    # Embedding Layer\n",
    "    embedding = Embedding(\n",
    "        input_dim=MAX_VOCAB_SIZE,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        input_length=MAX_SEQ_LENGTH\n",
    "    )(inputs)\n",
    "\n",
    "    # First Conv1D Layer\n",
    "    conv1 = Conv1D(\n",
    "        filters=hp.Int(\"filters_1\", min_value=64, max_value=256, step=64),\n",
    "        kernel_size=hp.Choice(\"kernel_size_1\", values=[3, 5, 7]),\n",
    "        activation='relu'\n",
    "    )(embedding)\n",
    "\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pooled1 = GlobalMaxPooling1D()(conv1)\n",
    "\n",
    "    # Second Conv1D Layer\n",
    "    conv2 = Conv1D(\n",
    "        filters=hp.Int(\"filters_2\", min_value=64, max_value=256, step=64),\n",
    "        kernel_size=hp.Choice(\"kernel_size_2\", values=[3, 5, 7]),\n",
    "        activation='relu'\n",
    "    )(embedding)  # Use embedding as input again\n",
    "\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pooled2 = GlobalMaxPooling1D()(conv2)\n",
    "\n",
    "    # Concatenate both pooled layers\n",
    "    merged = tf.keras.layers.Concatenate()([pooled1, pooled2])\n",
    "\n",
    "    dense = Dense(hp.Int(\"dense_units\", min_value=32,\n",
    "                  max_value=128, step=32), activation='relu')(merged)\n",
    "\n",
    "    dropout = Dropout(hp.Float(\"dropout_rate\", min_value=0.2,\n",
    "                      max_value=0.5, step=0.1))(dense)\n",
    "\n",
    "    outputs = Dense(len(label_dict), activation='softmax')(dropout)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=hp.Choice(\n",
    "            \"learning_rate\", values=[1e-2, 1e-3, 1e-4])),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Train Sequences Shape: {train_sequences.shape}\")  # DEBUG\n",
    "# print(f\"Train Labels Shape: {train_labels.shape}\")  # DEBUG\n",
    "\n",
    "# print(f\"Validation Sequences Shape: {val_sequences.shape}\")  # DEBUG\n",
    "# print(f\"Validation Labels Shape: {val_labels.shape}\")  # DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 Complete [00h 03m 12s]\n",
      "val_accuracy: 0.6784763932228088\n",
      "\n",
      "Best val_accuracy So Far: 0.6784763932228088\n",
      "Total elapsed time: 00h 15m 34s\n",
      "Best hyperparameters: {'filters_1': 256, 'kernel_size_1': 5, 'filters_2': 192, 'kernel_size_2': 3, 'dense_units': 128, 'dropout_rate': 0.2, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=3,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"models/hyperparameter_tuning\",\n",
    "    project_name=\"cnn_relation_extraction\"\n",
    ")\n",
    "\n",
    "tuner.search(train_sequences, train_labels, validation_data=(\n",
    "    val_sequences, val_labels), epochs=5, batch_size=32)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best hyperparameters: {best_hps.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 25ms/step - accuracy: 0.2580 - loss: 3.1798 - val_accuracy: 0.6094 - val_loss: 1.3763\n",
      "Epoch 2/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 26ms/step - accuracy: 0.6347 - loss: 1.2699 - val_accuracy: 0.6570 - val_loss: 1.2381\n",
      "Epoch 3/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 29ms/step - accuracy: 0.7299 - loss: 0.9043 - val_accuracy: 0.6758 - val_loss: 1.1832\n",
      "Epoch 4/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 27ms/step - accuracy: 0.7868 - loss: 0.6885 - val_accuracy: 0.6796 - val_loss: 1.2950\n",
      "Epoch 5/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 26ms/step - accuracy: 0.8160 - loss: 0.5818 - val_accuracy: 0.6662 - val_loss: 1.4156\n",
      "Epoch 6/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 26ms/step - accuracy: 0.8434 - loss: 0.4922 - val_accuracy: 0.6672 - val_loss: 1.5694\n",
      "Epoch 7/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 26ms/step - accuracy: 0.8623 - loss: 0.4434 - val_accuracy: 0.6791 - val_loss: 1.5312\n",
      "Epoch 8/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 25ms/step - accuracy: 0.8748 - loss: 0.4025 - val_accuracy: 0.6682 - val_loss: 1.6515\n",
      "Epoch 9/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 25ms/step - accuracy: 0.8840 - loss: 0.3631 - val_accuracy: 0.6728 - val_loss: 1.7706\n",
      "Epoch 10/10\n",
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 25ms/step - accuracy: 0.8906 - loss: 0.3442 - val_accuracy: 0.6745 - val_loss: 1.7916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x36239afd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "best_model.fit(\n",
    "    train_sequences, train_labels,\n",
    "    validation_data=(val_sequences, val_labels),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7042 - loss: 1.7563\n",
      "Test Accuracy: 0.6720\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = best_model.evaluate(test_sequences, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model and tokenizer saved successfully in 'models/' folder.\n"
     ]
    }
   ],
   "source": [
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "best_model.save(os.path.join(models_dir, \"cnn_relation_extraction.h5\"))\n",
    "\n",
    "with open(os.path.join(models_dir, \"tokenizer.json\"), \"w\") as f:\n",
    "    json.dump(tokenizer.word_index, f)\n",
    "\n",
    "print(\"Best model and tokenizer saved successfully in 'models/' folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
